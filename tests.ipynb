{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a siamese network to learn the similarity between two sentences\n",
    "# It uses BERT embeddings to encode the sentences\n",
    "# It need to support the triplet loss function\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "criteria = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        return self.sigmoid(self.fc(pooled_output))\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        return outputs[1]\n",
    "\n",
    "    def predict(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        return self.forward(input_ids1, attention_mask1), self.forward(input_ids2, attention_mask2)\n",
    "\n",
    "    def triplet_loss(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, margin):\n",
    "        # Compute the embeddings for the three inputs\n",
    "        emb1 = self.encode(input_ids1, attention_mask1)\n",
    "        emb2 = self.encode(input_ids2, attention_mask2)\n",
    "        emb3 = self.encode(input_ids3, attention_mask3)\n",
    "\n",
    "        # Compute the distances between the embeddings\n",
    "        dist_pos = nn.functional.pairwise_distance(emb1, emb2)\n",
    "        dist_neg = nn.functional.pairwise_distance(emb1, emb3)\n",
    "\n",
    "        # Compute the triplet loss\n",
    "        loss = nn.functional.relu(dist_pos - dist_neg + margin)\n",
    "\n",
    "        return loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3362, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "criteria = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    tokens = tokenizer(sentence, add_special_tokens=True, return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "anchor = \"Trucks are awesome\"\n",
    "positive = \"Pigs are cool animals\"\n",
    "negative = \"I like trucks\"\n",
    "\n",
    "input_ids1, attention_mask1 = encode_sentence(anchor)\n",
    "input_ids2, attention_mask2 = encode_sentence(positive)\n",
    "input_ids3, attention_mask3 = encode_sentence(negative)\n",
    "\n",
    "anchoer_emb = model(input_ids1, attention_mask1)[1] # Gets the pooled output\n",
    "positive_emb = model(input_ids2, attention_mask2)[1]\n",
    "negative_emb = model(input_ids3, attention_mask3)[1]\n",
    "\n",
    "loss = criteria(anchoer_emb, positive_emb, negative_emb)\n",
    "print(loss) # tensor(2.3362, grad_fn=<MeanBackward0>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
